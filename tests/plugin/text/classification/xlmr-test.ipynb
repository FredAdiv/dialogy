{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "import dialogy.constants as const\n",
    "from dialogy.base import Input\n",
    "from dialogy.plugins import MergeASROutputPlugin, XLMRMultiClass\n",
    "from dialogy.utils import load_file\n",
    "from dialogy.workflow import Workflow\n",
    "from tests import load_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>labels</th>\n",
       "      <th>state</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>_confirm_</td>\n",
       "      <td>state1</td>\n",
       "      <td>lang1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yea</td>\n",
       "      <td>_confirm_</td>\n",
       "      <td>state2</td>\n",
       "      <td>lang1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>_cancel_</td>\n",
       "      <td>state1</td>\n",
       "      <td>lang2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nope</td>\n",
       "      <td>_cancel_</td>\n",
       "      <td>state2</td>\n",
       "      <td>lang2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data     labels   state   lang\n",
       "0   yes  _confirm_  state1  lang1\n",
       "1   yea  _confirm_  state2  lang1\n",
       "2    no   _cancel_  state1  lang2\n",
       "3  nope   _cancel_  state2  lang2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_prompt = pd.DataFrame(\n",
    "    [\n",
    "        {\"data\": \"yes\", \"labels\": \"_confirm_\", \"state\": \"state1\", \"lang\": \"lang1\"},\n",
    "        {\"data\": \"yea\", \"labels\": \"_confirm_\", \"state\": \"state2\", \"lang\":\"lang1\"},\n",
    "        {\"data\": \"no\", \"labels\": \"_cancel_\", \"state\": \"state1\", \"lang\":\"lang2\"},\n",
    "        {\"data\": \"nope\", \"labels\": \"_cancel_\", \"state\": \"state2\", \"lang\":\"lang2\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "transcripts = [\"yes\"]\n",
    "intent = \"_confirm_\"\n",
    "state = \"state1\"\n",
    "lang = \"lang1\"\n",
    "\n",
    "train_df_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:14.812464+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L313 \u001b[34mtrain(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mAdding prompts to input text\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress bar:: 100%|██████████| 4/4 [00:00<00:00, 426.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:14.829306+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L340 \u001b[34mtrain(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mTraining dataset size (original): (4, 4)\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:14.830419+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L341 \u001b[34mtrain(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mTraining dataset size (after adding prompts): (4, 4)\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:28.577674+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L348 \u001b[34mtrain(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mDisplaying a few samples (this goes into the model):\n",
      "                                  text labels\n",
      "2    no<s> state1 </s><s> prompt2 </s>      0\n",
      "3  nope<s> state2 </s><s> prompt1 </s>      0\n",
      "0                   yea<s> state2 </s>      1\n",
      "1                  nope<s> state2 </s>      0\n",
      "Labels: 2.\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.9.2/envs/dialogy/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:475: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n",
      "/root/.pyenv/versions/3.9.2/envs/dialogy/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9957fbf446e544c48dbaac438b3e3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae76242892ff4343ac09a76ae075b6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = \"/root/amey/clients/vodafone/\"\n",
    "\n",
    "prompts_map = {\n",
    "    'lang1': {'state1': ['prompt1', 'prompt2', 'prompt3'],'state2': ['prompt1', 'prompt2']},\n",
    "    'lang2': {'state1': ['prompt1', 'prompt2'], 'state2': ['prompt1', 'prompt2']}\n",
    "}\n",
    "\n",
    "xlmr_clf_prompt = XLMRMultiClass(\n",
    "    model_dir=directory,\n",
    "    dest=\"output.intents\",\n",
    "    debug=True,\n",
    "    prompts_map=prompts_map,\n",
    "    use_state=True,\n",
    "    use_prompt=True,\n",
    "    train_using_all_prompts=True\n",
    ")\n",
    "\n",
    "merge_asr_output_plugin = MergeASROutputPlugin(\n",
    "    dest=\"input.clf_feature\", debug=False\n",
    ")\n",
    "\n",
    "xlmr_clf_prompt.train(train_df_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[31m\u001b[1mERROR\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:51.452809+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L257 \u001b[34mvalidate(...)\u001b[0m\n",
      "\u001b[31m\u001b[1mTraining dataframe is empty.\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[33m\u001b[1mWARNING\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:51.455389+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L271 \u001b[34mvalidate(...)\u001b[0m\n",
      "\u001b[33m\u001b[1mColumn data not found in training data\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:51.456873+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier input:\n",
      "['text']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:51.458407+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier input:\n",
      "['yes', 'yeah']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:11:51.459694+0530\u001b[0m\n",
      "FILE: dialogy.plugins.text.classification.xlmr:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier input:\n",
      "['yes', 'yeah']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transcripts = [\"yes\"]\n",
    "intent = \"_confirm_\"\n",
    "state = \"state1\"\n",
    "lang = \"lang1\"\n",
    "\n",
    "train_df_empty = pd.DataFrame()\n",
    "train_df_invalid = pd.DataFrame(\n",
    "    [\n",
    "        {\"apples\": \"yes\", \"fruit\": \"fruit\"},\n",
    "        {\"apples\": \"yea\", \"fruit\": \"fruit\"},\n",
    "        {\"apples\": \"no\", \"fruit\": \"fruit\"},\n",
    "        {\"apples\": \"nope\", \"fruit\": \"fruit\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert xlmr_clf_prompt.validate(train_df_empty) is False\n",
    "assert xlmr_clf_prompt.validate(train_df_invalid) is False\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"text\"])\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"yes\",\"yeah\"], state=\"WORKING STATE\")\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"yes\",\"yeah\"], state=None, lang=\"hi\")\n",
    "\n",
    "# with pytest.raises(ValueError):\n",
    "#     xlmr_clf_prompt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr_clf_prompt.inference(texts=[\"yes\"], state=\"state1\", lang=\"lang1\")\n",
    "# xlmr_clf_prompt.use_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining XLMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ".. _xlmr_classifier:\n",
    "\n",
    "This module provides a trainable XLMR classifier.\n",
    "[read-more](https://arxiv.org/abs/1911.02116)\n",
    "\"\"\"\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import dialogy.constants as const\n",
    "from dialogy.base import Guard, Input, Output, Plugin\n",
    "from dialogy.types import Intent\n",
    "from dialogy.utils import load_file, logger, read_from_json, save_file\n",
    "\n",
    "\n",
    "class XLMRMultiClass(Plugin):\n",
    "    \"\"\"\n",
    "    This plugin provides a classifier based on `XLM-Roberta <https://arxiv.org/abs/1911.02116>`.\n",
    "\n",
    "    .. _XLMRMultiClass:\n",
    "    The use_state flag in the XLMRMultiClass plugin is used to enable the use of state variable as the part of the text input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dir: str,\n",
    "        dest: Optional[str] = None,\n",
    "        guards: Optional[List[Guard]] = None,\n",
    "        debug: bool = False,\n",
    "        threshold: float = 0.1,\n",
    "        use_cuda: bool = False,\n",
    "        score_round_off: int = 5,\n",
    "        purpose: str = const.TRAIN,\n",
    "        fallback_label: str = const.ERROR_LABEL,\n",
    "        use_state: bool = False,\n",
    "        data_column: str = const.DATA,\n",
    "        label_column: str = const.LABELS,\n",
    "        state_column: str = const.STATE,\n",
    "        lang_column: str = const.LANG,\n",
    "        args_map: Optional[Dict[str, Any]] = None,\n",
    "        skip_labels: Optional[List[str]] = None,\n",
    "        prompts_map: dict = None,\n",
    "        use_prompt: bool = False,\n",
    "        train_using_all_prompts: bool = True,\n",
    "        null_prompt_token: str = const.NULL_PROMPT_TOKEN,\n",
    "        kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        try:\n",
    "            classifer = getattr(\n",
    "                importlib.import_module(const.XLMR_MODULE), const.XLMR_MULTI_CLASS_MODEL\n",
    "            )\n",
    "        except ModuleNotFoundError as error:\n",
    "            raise ModuleNotFoundError(\n",
    "                \"Plugin requires simpletransformers -- https://simpletransformers.ai/docs/installation/\"\n",
    "            ) from error\n",
    "\n",
    "        super().__init__(dest=dest, guards=guards, debug=debug)\n",
    "        self.labelencoder = preprocessing.LabelEncoder()\n",
    "        self.classifier = classifer\n",
    "        self.model: Any = None\n",
    "        self.model_dir = model_dir\n",
    "        self.fallback_label = fallback_label\n",
    "        self.data_column = data_column\n",
    "        self.label_column = label_column\n",
    "        self.state_column = state_column\n",
    "        self.lang_column = lang_column\n",
    "        self.use_cuda = use_cuda\n",
    "        self.use_state = use_state\n",
    "        self.labelencoder_file_path = os.path.join(\n",
    "            self.model_dir, const.LABELENCODER_FILE\n",
    "        )\n",
    "        self.ts_parameter: float = (\n",
    "            read_from_json(\n",
    "                [const.TS_PARAMETER], model_dir, const.CALIBRATION_CONFIG_FILE\n",
    "            ).get(const.TS_PARAMETER)\n",
    "            or 1.0\n",
    "        )\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.skip_labels = set(skip_labels or set())\n",
    "        self.purpose = purpose\n",
    "        self.round = score_round_off\n",
    "        if args_map and (\n",
    "            const.TRAIN not in args_map\n",
    "            or const.TEST not in args_map\n",
    "            or const.PRODUCTION not in args_map\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Attempting to set invalid {args_map}. \"\n",
    "                f\"It is missing some of {const.TRAIN}, {const.TEST}, {const.PRODUCTION} in configs.\"\n",
    "            )\n",
    "        self.args_map = args_map\n",
    "        self.kwargs = kwargs or {}\n",
    "        try:\n",
    "            if os.path.exists(self.labelencoder_file_path):\n",
    "                self.init_model()\n",
    "        except EOFError:\n",
    "            logger.error(\n",
    "                f\"Plugin {self} Failed to load labelencoder from {self.labelencoder_file_path}. \"\n",
    "                \"Ignore this message if you are training but if you are using this in production or testing, then this is serious!\"\n",
    "            )\n",
    "        self.prompts_map = prompts_map\n",
    "        self.use_prompt = use_prompt\n",
    "        self.train_using_all_prompts = train_using_all_prompts\n",
    "        self.null_prompt_token = null_prompt_token\n",
    "        self.debug = debug\n",
    "\n",
    "    def init_model(self, label_count: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model if artifacts are available.\n",
    "\n",
    "        :param label_count: number of labels to train on or predict, defaults to None\n",
    "        :type label_count: Optional[int], optional\n",
    "        :raises ValueError: In case n is not provided or can't be calculated.\n",
    "        \"\"\"\n",
    "        model_dir = const.XLMR_MODEL_TIER\n",
    "        if os.path.exists(self.labelencoder_file_path):\n",
    "            self.load()\n",
    "            label_count = len(self.labelencoder.classes_)\n",
    "            model_dir = self.model_dir\n",
    "        if not label_count:\n",
    "            raise ValueError(\n",
    "                f\"Plugin {self} needs either the training data \"\n",
    "                \"or an existing labelencoder to initialize.\"\n",
    "            )\n",
    "        args = (\n",
    "            self.args_map[self.purpose]\n",
    "            if self.args_map and self.purpose in self.args_map\n",
    "            else {}\n",
    "        )\n",
    "        self.use_calibration = args.get(const.MODEL_CALIBRATION)\n",
    "        try:\n",
    "            self.model = self.classifier(\n",
    "                const.XLMR_MODEL,\n",
    "                model_dir,\n",
    "                num_labels=label_count,\n",
    "                use_cuda=self.use_cuda,\n",
    "                args=args,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "        except OSError:\n",
    "            self.model = self.classifier(\n",
    "                const.XLMR_MODEL,\n",
    "                const.XLMR_MODEL_TIER,\n",
    "                num_labels=label_count,\n",
    "                use_cuda=self.use_cuda,\n",
    "                args=args,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def valid_labelencoder(self) -> bool:\n",
    "        return hasattr(self.labelencoder, \"classes_\")\n",
    "\n",
    "    def inference(\n",
    "        self, texts: Optional[List[str]], state: Optional[str] = None, lang: Optional[str] = None,\n",
    "    ) -> List[Intent]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict the intent of a list of texts.\n",
    "        If the model has been trained using the state features, it expects the text to also be appended with the state token else the predictions would be spurious.\n",
    "\n",
    "        :param texts: A list of strings, derived from ASR transcripts.\n",
    "        :param state: state, mapped to the ASR transcripts.\n",
    "        :type texts: List[str]\n",
    "        :type state: List[str]\n",
    "        :raises AttributeError: In case the labelencoder is not available.\n",
    "        :return: A list of intents corresponding to texts.\n",
    "        :rtype: List[Intent]\n",
    "        \"\"\"\n",
    "        logger.debug(f\"Classifier Input:\\n{texts}\")\n",
    "        \n",
    "        fallback_output = Intent(name=self.fallback_label, score=1.0).add_parser(self)\n",
    "        if not texts:\n",
    "            logger.error(f\"texts passed to model {texts}!\")\n",
    "            return [fallback_output]\n",
    "\n",
    "        if self.model is None:\n",
    "            logger.error(f\"No model found for plugin {self.__class__.__name__}!\")\n",
    "            return [fallback_output]\n",
    "\n",
    "        if self.use_state and not state:\n",
    "            raise ValueError(\n",
    "                f\"Plugin {self.__class__.__name__} requires state to be passed to the model.\"\n",
    "            )\n",
    "        \n",
    "        if self.use_prompt and not state:\n",
    "            raise ValueError(\n",
    "                f\"In order to use prompts as feature, Plugin {self.__class__.__name__} is requires state to be passed to the model.\"\n",
    "            )\n",
    "\n",
    "        if self.use_prompt and not lang:\n",
    "            raise ValueError(\n",
    "                f\"In order to use prompts as feature, Plugin {self.__class__.__name__} is requires lang to be passed to the model.\"\n",
    "            )\n",
    "\n",
    "        elif self.use_prompt and state:\n",
    "            texts[0] += \"<s> \" + self.lookup_prompt(lang, state)[0] + \" </s>\"\n",
    "            if self.debug:\n",
    "                logger.debug(f\"Classification input (Prompt as a feature enabled):\\n{texts}\")\n",
    "                logger.debug(texts)\n",
    "                # logger.debug(self.lookup_prompt(lang, state)[0])\n",
    "\n",
    "        elif self.use_state and state:\n",
    "            texts[0] += \"<s> \" + state + \" </s>\"\n",
    "            if self.debug:\n",
    "                logger.debug(texts)\n",
    "                logger.debug(f\"Classifition input (State as a feature enabled):\\n{texts}\")\n",
    "\n",
    "\n",
    "        if not self.valid_labelencoder:\n",
    "            raise AttributeError(\n",
    "                \"Seems like you forgot to \"\n",
    "                f\"save the {self.__class__.__name__} plugin.\"\n",
    "            )\n",
    "            \n",
    "        predictions, logits = self.model.predict(texts)\n",
    "        if not predictions:\n",
    "            return [fallback_output]\n",
    "\n",
    "        logits = logits / self.ts_parameter\n",
    "        confidence_scores = [np.exp(logit) / sum(np.exp(logit)) for logit in logits]\n",
    "        intents_confidence_order = np.argsort(confidence_scores)[0][::-1]\n",
    "        predicted_intents = self.labelencoder.inverse_transform(\n",
    "            intents_confidence_order\n",
    "        )\n",
    "        ordered_confidence_scores = [\n",
    "            confidence_scores[0][idx] for idx in intents_confidence_order\n",
    "        ]\n",
    "\n",
    "        if self.use_calibration:\n",
    "            ordered_confidence_scores = [\n",
    "                logits[0][idx] for idx in np.argsort(logits)[0][::-1]\n",
    "            ]  # ordered logits for calibration\n",
    "\n",
    "        return [\n",
    "            Intent(name=intent, score=round(score, self.round)).add_parser(self)\n",
    "            for intent, score in zip(predicted_intents, ordered_confidence_scores)\n",
    "        ]\n",
    "\n",
    "    def validate(self, training_data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate the training data is in the appropriate format\n",
    "\n",
    "        :param training_data: A pandas dataframe containing at least list of strings and corresponding labels.\n",
    "            Should also contain a state key if use_state = True while initializing object.\n",
    "        :type training_data: pd.DataFrame\n",
    "        :return: True if the dataframe is valid, False otherwise.\n",
    "        :rtype: bool\n",
    "        \"\"\"\n",
    "        if training_data.empty:\n",
    "            logger.error(\"Training dataframe is empty.\")\n",
    "            return False\n",
    "        expected_columns = [self.data_column, self.label_column]\n",
    "\n",
    "        if self.use_prompt:\n",
    "            expected_columns.append(self.lang_column)\n",
    "            expected_columns.append(self.state_column)\n",
    "\n",
    "        if self.use_state:\n",
    "            if self.state_column not in expected_columns:\n",
    "                expected_columns.append(self.state_column)\n",
    "        \n",
    "        for column in expected_columns:\n",
    "            if column not in training_data.columns:\n",
    "                logger.warning(f\"Column {column} not found in training data\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def train(self, training_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Train an intent-classifier on the provided training data.\n",
    "\n",
    "        The training is skipped if the data-format is not valid.\n",
    "        While training with the use_state flag as true, make sure that the state column is the part of the training_data dataframe\n",
    "        :param training_data: A pandas dataframe containing at least list of strings and corresponding labels.\n",
    "        :type training_data: pd.DataFrame\n",
    "        \"\"\"\n",
    "        if not self.validate(training_data):\n",
    "            logger.warning(\n",
    "                f\"Training dataframe is invalid, for {self.__class__.__name__} plugin.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        skip_labels_filter = training_data[self.label_column].isin(self.skip_labels)\n",
    "        training_data = training_data[~skip_labels_filter].copy()\n",
    "\n",
    "        encoder = self.labelencoder.fit(training_data[self.label_column])\n",
    "\n",
    "        sample_size = 5 if len(training_data) > 5 else len(training_data)\n",
    "        training_data.rename(\n",
    "            columns={self.data_column: const.TEXT, self.label_column: const.LABELS},\n",
    "            inplace=True,\n",
    "        )\n",
    "        training_data.loc[:, const.LABELS] = encoder.transform(\n",
    "            training_data[const.LABELS]\n",
    "        )\n",
    "\n",
    "        # Append state to text\n",
    "        if self.use_state:\n",
    "            # logger.debug(training_data[const.STATE])\n",
    "            training_data[const.TEXT] += \"<s> \" + training_data[self.state_column] + \" </s>\"\n",
    "\n",
    "        # Append prompt to text\n",
    "        if self.use_prompt:\n",
    "            training_data_ = training_data.copy().reset_index(drop=True)\n",
    "\n",
    "            logger.debug(\"Adding prompts to input text\")\n",
    "            for i in tqdm(range(training_data.shape[0]), desc=\"progress bar:\"):\n",
    "                _lang = training_data.iloc[i][self.lang_column]\n",
    "                _state = training_data.iloc[i][self.state_column]\n",
    "                \n",
    "                if not self.train_using_all_prompts:\n",
    "                    _prompt =  self.lookup_prompt(_lang, _state)[0]\n",
    "                    training_data_.at[i, const.TEXT] = training_data.iloc[i][const.TEXT] \\\n",
    "                            + \"<s> \" \\\n",
    "                            + _prompt \\\n",
    "                            + \" </s>\"\n",
    "                else:\n",
    "                    _prompts =  self.lookup_prompt(_lang, _state, return_all=True)\n",
    "                    for prompt  in _prompts:\n",
    "                        _row = pd.DataFrame(training_data.iloc[i]).T.reset_index(drop=True)\n",
    "                        _row.at[0,const.TEXT] = training_data.iloc[i][const.TEXT] \\\n",
    "                            + \"<s> \" \\\n",
    "                            + prompt \\\n",
    "                            + \" </s>\"\n",
    "                        training_data_ = pd.concat([training_data_, _row])\n",
    "\n",
    "                    if i in training_data_.index:\n",
    "                        training_data_ = training_data_.drop(i, axis=0)\n",
    "\n",
    "                    training_data_ = training_data_.reset_index(drop=True)\n",
    "            \n",
    "            if self.train_using_all_prompts:\n",
    "                logger.debug(f\"Training dataset size (original): {training_data.shape}\")\n",
    "                logger.debug(f\"Training dataset size (after adding prompts): {training_data_.shape}\")\n",
    "\n",
    "            training_data = training_data_\n",
    "            del training_data_\n",
    "                    \n",
    "        training_data = training_data[[const.TEXT, const.LABELS]]\n",
    "        self.init_model(len(encoder.classes_))\n",
    "        logger.debug(\n",
    "            f\"Displaying a few samples (this goes into the model):\\n{training_data.sample(sample_size)}\\nLabels: {len(encoder.classes_)}.\"\n",
    "        )\n",
    "\n",
    "        self.model.train_model(training_data)\n",
    "        self.save()\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the plugin artifacts.\n",
    "\n",
    "        :raises ValueError: In case the labelencoder is not trained.\n",
    "        \"\"\"\n",
    "        if not self.model or not self.valid_labelencoder:\n",
    "            raise ValueError(\n",
    "                f\"Plugin {self.__class__.__name__} seems to be un-trained.\"\n",
    "            )\n",
    "        save_file(\n",
    "            self.labelencoder_file_path,\n",
    "            self.labelencoder,\n",
    "            mode=\"wb\",\n",
    "            writer=pickle.dump,\n",
    "        )\n",
    "\n",
    "    def lookup_prompt(self, lang: str, state: str, return_all: bool = False) -> List[str]:\n",
    "        \"\"\"\n",
    "        Same as get_prompt() method, but built for faster lookup to reduce latency during inference. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return random.sample(self.prompts_map.get(lang).get(state), 1)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                logger.debug(e)\n",
    "                logger.debug(f\"Prompt not found for Lang: {lang} \\t State: {state}\")\n",
    "            return [self.null_prompt_token]\n",
    "            \n",
    "    def load(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the plugin artifacts.\n",
    "        \"\"\"\n",
    "        self.labelencoder = load_file(\n",
    "            self.labelencoder_file_path, mode=\"rb\", loader=pickle.load\n",
    "        )\n",
    "\n",
    "    def utility(self, input: Input, _: Output) -> List[Intent]:\n",
    "        return ( \n",
    "        self.inference(input.clf_feature, input.current_state, input.lang)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialogy.plugins import MergeASROutputPlugin\n",
    "\n",
    "train_df_prompt = pd.DataFrame(\n",
    "    [\n",
    "        {\"data\": \"yes\", \"labels\": \"_confirm_\", \"state\": \"state1\", \"lang\": \"lang1\"},\n",
    "        {\"data\": \"yea\", \"labels\": \"_confirm_\", \"state\": \"state2\", \"lang\":\"lang1\"},\n",
    "        {\"data\": \"no\", \"labels\": \"_cancel_\", \"state\": \"state1\", \"lang\":\"lang2\"},\n",
    "        {\"data\": \"nope\", \"labels\": \"_cancel_\", \"state\": \"state2\", \"lang\":\"lang2\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "transcripts = [\"yes\"]\n",
    "intent = \"_confirm_\"\n",
    "state = \"state1\"\n",
    "lang = \"lang1\"\n",
    "\n",
    "train_df_prompt.head()\n",
    "\n",
    "directory = \"/root/amey/clients/vodafone/\"\n",
    "\n",
    "prompts_map = {\n",
    "    'lang1': {'state1': ['prompt1', 'prompt2', 'prompt3'],'state2': ['prompt1', 'prompt2']},\n",
    "    'lang2': {'state1': ['prompt1', 'prompt2'], 'state2': ['prompt1', 'prompt2']}\n",
    "}\n",
    "\n",
    "xlmr_clf_prompt = XLMRMultiClass(\n",
    "    model_dir=directory,\n",
    "    dest=\"output.intents\",\n",
    "    debug=True,\n",
    "    prompts_map=prompts_map,\n",
    "    use_state=True,\n",
    "    use_prompt=True,\n",
    "    train_using_all_prompts=True\n",
    ")\n",
    "\n",
    "merge_asr_output_plugin = MergeASROutputPlugin(\n",
    "    dest=\"input.clf_feature\", debug=False\n",
    ")\n",
    "\n",
    "xlmr_clf_prompt.train(train_df_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:25:37.809459+0530\u001b[0m\n",
      "FILE: __main__:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier Input:\n",
      "['yes']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:25:37.814474+0530\u001b[0m\n",
      "FILE: __main__:L208 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassification input (Prompt as a feature enabled):\n",
      "['yes<s> prompt3 </s>']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:25:37.815916+0530\u001b[0m\n",
      "FILE: __main__:L209 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1m['yes<s> prompt3 </s>']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4312b4eeab497cac8a1d04be9ed560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf45734c4fb49d5bc4d57bd9cad8ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Intent(name='_confirm_', score=0.56808, alternative_index=None),\n",
       " Intent(name='_cancel_', score=0.43192, alternative_index=None)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr_clf_prompt.inference(texts=[\"yes\"], state=\"state1\", lang=\"lang1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:30:16.424578+0530\u001b[0m\n",
      "FILE: __main__:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier Input:\n",
      "['text']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:30:16.427576+0530\u001b[0m\n",
      "FILE: __main__:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier Input:\n",
      "['text']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "\u001b[34m\u001b[1mDEBUG\u001b[0m\n",
      "-------\n",
      "TIME: \u001b[32m2022-07-21T03:30:16.428964+0530\u001b[0m\n",
      "FILE: __main__:L179 \u001b[34minference(...)\u001b[0m\n",
      "\u001b[34m\u001b[1mClassifier Input:\n",
      "['text']\u001b[0m\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"text\"])\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"text\"], state=\"state1\")\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"text\"], lang=\"lang2\")\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    xlmr_clf_prompt.inference(texts=[\"text\"], lang=\"lang2\")\n",
    "    \n",
    "# with pytest.raises(ValueError):\n",
    "#     xlmr_clf_prompt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('dialogy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9603ae628a3a0cfb02c984d3a545109ab50fe59b71dcf16acb110f5a863fceb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
